# MedLlama

Need to run the code directly? All you need is docker, k8s, and an L4 GPU! No other requirements.

For more information on the project and the code, check out the detailed article at https://medium.com/@harshithbelagur/is-fine-tuning-an-llm-for-a-downstream-task-always-a-good-idea-12ae8e3971df
